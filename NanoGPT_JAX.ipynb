{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPfusmQWZURhf10cwjOKbaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubermenchh/jax-sandbox/blob/main/NanoGPT_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax flax"
      ],
      "metadata": {
        "id": "txsisojcLA8p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JmZz6LpnKl0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f464c76a-b27b-4324-ca6d-9b0cbdcb4459"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "\n",
        "from typing import Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.profiler"
      ],
      "metadata": {
        "id": "2BJJ-rOoatEm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    vocab_size: int = 50257\n",
        "    block_size: int = 128\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    lr: float = 3e-4\n",
        "    max_iters: int = 10\n",
        "    batch_size: int = 8\n",
        "    grad_clip: float = 1.0"
      ],
      "metadata": {
        "id": "T7972CskKroC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nnx.Module):\n",
        "    def __init__(self, config, rngs=nnx.Rngs(0)) -> None:\n",
        "        self.config = config\n",
        "        assert self.config.n_embd % self.config.n_head == 0\n",
        "\n",
        "        self.c_attn = nnx.Linear(self.config.n_embd, 3 * self.config.n_embd, rngs=rngs)\n",
        "        self.c_proj = nnx.Linear(self.config.n_embd, self.config.n_embd, rngs=rngs)\n",
        "\n",
        "        self.n_head = self.config.n_head\n",
        "        self.n_embd = self.config.n_embd\n",
        "        self.head_dim = self.n_embd // self.n_head\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        B, T, C = x.shape # batch_size, seq_len, embed_dim\n",
        "\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
        "\n",
        "        def reshape_heads(x):\n",
        "            return x.reshape(B, T, self.n_head, self.head_dim).transpose(0, 2, 1, 3)\n",
        "\n",
        "        q = reshape_heads(q)\n",
        "        k = reshape_heads(k)\n",
        "        v = reshape_heads(v)\n",
        "\n",
        "        # Scaled Dot Product Attention\n",
        "        scale = -1.0 / jnp.sqrt(self.head_dim)\n",
        "        scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) * scale\n",
        "\n",
        "        # Causal mask\n",
        "        mask = jnp.tril(jnp.ones((T, T)))\n",
        "        scores = jnp.where(mask == 0, float(\"-inf\"), scores)\n",
        "\n",
        "        # Attention Weights\n",
        "        weights = jax.nn.softmax(scores, axis=-1)\n",
        "        # Attention output\n",
        "        out = jnp.matmul(weights, v)\n",
        "        out = out.transpose(0, 2, 1, 3).reshape(B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        return self.c_proj(out)"
      ],
      "metadata": {
        "id": "6rMkySl6KtG1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nnx.Module):\n",
        "    def __init__(self, config: Config, rngs=nnx.Rngs(0)) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.c_fc = nnx.Linear(config.n_embd, 4 * config.n_embd, rngs=rngs)\n",
        "        self.c_proj = nnx.Linear(config.n_embd * 4, config.n_embd, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        x = self.c_fc(x)\n",
        "        x = jax.nn.gelu(x, approximate=True)\n",
        "        x = self.c_proj(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BELRrYShKw5O"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nnx.Module):\n",
        "    def __init__(self, config: Config, rngs=nnx.Rngs(0)) -> None:\n",
        "        super().__init__()\n",
        "        self.ln_1 = nnx.LayerNorm(num_features=config.n_embd, rngs=rngs)\n",
        "        self.attn = MultiHeadAttention(config, rngs)\n",
        "        self.ln_2 = nnx.LayerNorm(num_features=config.n_embd, rngs=rngs)\n",
        "        self.mlp = MLP(config, rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "3MvMuN4SK0lu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nnx.Module):\n",
        "    def __init__(self, config: Config, rngs=nnx.Rngs(0)) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.wte = nnx.Embed(\n",
        "            num_embeddings=config.vocab_size,\n",
        "            features=config.n_embd,\n",
        "            param_dtype=jnp.float32,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.wpe = nnx.Embed(\n",
        "            num_embeddings=config.block_size,\n",
        "            features=config.n_embd,\n",
        "            param_dtype=jnp.float32,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.dropout = nnx.Dropout(rate=0.1)\n",
        "        self.blocks = [Block(config, rngs) for _ in range(config.n_layer)]\n",
        "        self.ln_f = nnx.LayerNorm(num_features=config.n_embd, rngs=rngs)\n",
        "        self.lm_head = nnx.Linear(\n",
        "            config.n_embd,\n",
        "            config.vocab_size,\n",
        "            use_bias=False,\n",
        "            kernel_init=nnx.initializers.normal(stddev=0.02),\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "    def __call__(self,\n",
        "                 idx: jnp.ndarray,\n",
        "                 deterministic: bool=True,\n",
        "                 targets: Optional[jnp.ndarray]=None) -> Tuple[jnp.ndarray, Optional[jnp.ndarray]]:\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        pos = jnp.arange(0, T, dtype=jnp.int32)\n",
        "\n",
        "        token_emb = self.wte(idx)\n",
        "        pos_emb = self.wpe(pos)\n",
        "\n",
        "        x = self.dropout(token_emb + pos_emb, deterministic=deterministic)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "                logits.reshape(-1, logits.shape[-1]),\n",
        "                targets.reshape(-1)\n",
        "            ).mean()\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None"
      ],
      "metadata": {
        "id": "RnRWr1UJKx4a"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "rngs = nnx.Rngs(0)\n",
        "gpt = GPT(config, rngs)"
      ],
      "metadata": {
        "id": "KIaIe4isLTbC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yUbFL3eLdZ-",
        "outputId": "1bba1677-1a6e-4faa-d7e0-1ffab02e58cb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "  wte=Embed(\n",
              "    embedding=Param(\n",
              "      value=Array(shape=(50257, 768), dtype=float32)\n",
              "    ),\n",
              "    num_embeddings=50257,\n",
              "    features=768,\n",
              "    dtype=dtype('float32'),\n",
              "    param_dtype=<class 'jax.numpy.float32'>,\n",
              "    embedding_init=<function variance_scaling.<locals>.init at 0x791b0b813250>\n",
              "  ),\n",
              "  wpe=Embed(\n",
              "    embedding=Param(\n",
              "      value=Array(shape=(128, 768), dtype=float32)\n",
              "    ),\n",
              "    num_embeddings=128,\n",
              "    features=768,\n",
              "    dtype=dtype('float32'),\n",
              "    param_dtype=<class 'jax.numpy.float32'>,\n",
              "    embedding_init=<function variance_scaling.<locals>.init at 0x791b0b813250>\n",
              "  ),\n",
              "  dropout=Dropout(rate=0.1, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=None),\n",
              "  blocks=[Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  ), Block(\n",
              "    ln_1=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    attn=MultiHeadAttention(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_attn=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 2304), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(2304,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=2304,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      n_head=12,\n",
              "      n_embd=768,\n",
              "      head_dim=64\n",
              "    ),\n",
              "    ln_2=LayerNorm(\n",
              "      scale=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      bias=Param(\n",
              "        value=Array(shape=(768,), dtype=float32)\n",
              "      ),\n",
              "      num_features=768,\n",
              "      epsilon=1e-06,\n",
              "      dtype=None,\n",
              "      param_dtype=<class 'jax.numpy.float32'>,\n",
              "      use_bias=True,\n",
              "      use_scale=True,\n",
              "      bias_init=<function zeros at 0x791b3845aef0>,\n",
              "      scale_init=<function ones at 0x791b3845b0a0>,\n",
              "      reduction_axes=-1,\n",
              "      feature_axes=-1,\n",
              "      axis_name=None,\n",
              "      axis_index_groups=None,\n",
              "      use_fast_variance=True\n",
              "    ),\n",
              "    mlp=MLP(\n",
              "      config=Config(vocab_size=50257, block_size=128, n_layer=12, n_head=12, n_embd=768, lr=0.0003, max_iters=10, batch_size=8, grad_clip=1.0),\n",
              "      c_fc=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(768, 3072), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(3072,), dtype=float32)\n",
              "        ),\n",
              "        in_features=768,\n",
              "        out_features=3072,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      ),\n",
              "      c_proj=Linear(\n",
              "        kernel=Param(\n",
              "          value=Array(shape=(3072, 768), dtype=float32)\n",
              "        ),\n",
              "        bias=Param(\n",
              "          value=Array(shape=(768,), dtype=float32)\n",
              "        ),\n",
              "        in_features=3072,\n",
              "        out_features=768,\n",
              "        use_bias=True,\n",
              "        dtype=None,\n",
              "        param_dtype=<class 'jax.numpy.float32'>,\n",
              "        precision=None,\n",
              "        kernel_init=<function variance_scaling.<locals>.init at 0x791b0b8127a0>,\n",
              "        bias_init=<function zeros at 0x791b3845aef0>,\n",
              "        dot_general=<function dot_general at 0x791b403005e0>\n",
              "      )\n",
              "    )\n",
              "  )],\n",
              "  ln_f=LayerNorm(\n",
              "    scale=Param(\n",
              "      value=Array(shape=(768,), dtype=float32)\n",
              "    ),\n",
              "    bias=Param(\n",
              "      value=Array(shape=(768,), dtype=float32)\n",
              "    ),\n",
              "    num_features=768,\n",
              "    epsilon=1e-06,\n",
              "    dtype=None,\n",
              "    param_dtype=<class 'jax.numpy.float32'>,\n",
              "    use_bias=True,\n",
              "    use_scale=True,\n",
              "    bias_init=<function zeros at 0x791b3845aef0>,\n",
              "    scale_init=<function ones at 0x791b3845b0a0>,\n",
              "    reduction_axes=-1,\n",
              "    feature_axes=-1,\n",
              "    axis_name=None,\n",
              "    axis_index_groups=None,\n",
              "    use_fast_variance=True\n",
              "  ),\n",
              "  lm_head=Linear(\n",
              "    kernel=Param(\n",
              "      value=Array(shape=(768, 50257), dtype=float32)\n",
              "    ),\n",
              "    bias=Param(\n",
              "      value=None\n",
              "    ),\n",
              "    in_features=768,\n",
              "    out_features=50257,\n",
              "    use_bias=False,\n",
              "    dtype=None,\n",
              "    param_dtype=<class 'jax.numpy.float32'>,\n",
              "    precision=None,\n",
              "    kernel_init=<function normal.<locals>.init at 0x7911c00db6d0>,\n",
              "    bias_init=<function zeros at 0x791b3845aef0>,\n",
              "    dot_general=<function dot_general at 0x791b403005e0>\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zzV46khiLys",
        "outputId": "526991af-0944-448a-9fbc-35e19668067a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-14 16:13:22--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-11-14 16:13:22 (17.9 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = open(\"input.txt\", \"r\").read()\n",
        "print(data[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deShV5wliQsF",
        "outputId": "13f518ea-08c3-42a8-d228-c6c64f60797a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "bpupHaPEihdu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "tsl12J-TiYH4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tokens = tokenizer.encode(data)\n",
        "n = len(tokens)\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lhv28j7inVB",
        "outputId": "e6f9b1a2-da01-4f58-ed26-ad46cfa0e8c1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "338025"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tokens[:int(n * 0.9)]\n",
        "valid_data = tokens[int(n * 0.9):int(n * 0.95)]\n",
        "test_data = tokens[int(n * 0.95):]\n",
        "\n",
        "len(train_data), len(valid_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hN2a53Xiv04",
        "outputId": "7e0c272d-748b-4c26-c218-8ee129a85490"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(304222, 16901, 16902)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[:10])\n",
        "print(tokenizer.decode(train_data[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_1-8_cljFsu",
        "outputId": "fbefe8f6-00b8-4b34-ee52-286a2479776c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11]\n",
            "First Citizen:\n",
            "Before we proceed any further,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx: idx + self.block_size + 1]\n",
        "        x = chunk[:-1]\n",
        "        y = chunk[1:]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "IHn-pIMJjZc_"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def jax_collate(batch):\n",
        "    max_len = max(len(item[0]) for item in batch)\n",
        "\n",
        "    inp = [item[0] + [0] * (max_len - len(item[0])) for item in batch]\n",
        "    trg = [item[1] + [0] * (max_len - len(item[1])) for item in batch]\n",
        "\n",
        "    inp, trg = np.array(inp), np.array(trg)\n",
        "    return inp, trg"
      ],
      "metadata": {
        "id": "yAhJswKO34hx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(train_data, config.block_size)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True, collate_fn=jax_collate, num_workers=4)\n",
        "\n",
        "valid_dataset = TextDataset(valid_data, config.block_size)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, drop_last=True, collate_fn=jax_collate, num_workers=4)\n",
        "\n",
        "test_dataset = TextDataset(test_data, config.block_size)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, drop_last=True, collate_fn=jax_collate, num_workers=4)"
      ],
      "metadata": {
        "id": "U7M6fTiFkRWF"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset) // config.batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAS8wdhU6898",
        "outputId": "36af677c-e369-4dce-fb2a-183037fe2e74"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38027"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = next(iter(valid_dataloader))"
      ],
      "metadata": {
        "id": "YeszmQYk3TC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8ea066-351b-44f0-8f5c-ad594161afb0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b[0].shape, b[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2ETkzVH5CQh",
        "outputId": "9ee8819e-fe1c-442d-8ab5-2042c365ba9b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 128), (8, 128))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-4\n",
        "weight_decay = 0.1\n",
        "\n",
        "optimizer = nnx.Optimizer(gpt, optax.adamw(learning_rate=lr, b1=0.9, b2=0.95, eps=1e-8, weight_decay=weight_decay))\n",
        "metrics = nnx.MultiMetric(loss=nnx.metrics.Average(\"loss\"), perplexity=nnx.metrics.Average(\"perplexity\"))"
      ],
      "metadata": {
        "id": "wLyjSjbSCgE4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model, batch):\n",
        "    inp, trg = batch\n",
        "    trg = trg.astype(int)\n",
        "    logits, loss = model(inp, deterministic=True, targets=trg)\n",
        "    return loss, logits"
      ],
      "metadata": {
        "id": "OSL_Pe42Ph0H"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def train_step(model, optimizer, metrics, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    perplexity = jnp.exp(loss)\n",
        "    metrics.update(loss=loss, perplexity=perplexity)\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "Tl7seT7DPsfV"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def eval_step(model: GPT, metrics: nnx.MultiMetric, batch):\n",
        "    loss, logits = loss_fn(model, batch)\n",
        "    perplexity = jnp.exp(loss)\n",
        "    metrics.update(loss=loss, perplexity=perplexity)"
      ],
      "metadata": {
        "id": "PvQhUTSuQJWj"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def generate(\n",
        "    model: GPT,\n",
        "    idx: jnp.ndarray,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: Optional[int] = None\n",
        "):\n",
        "    \"\"\"Generate text tokens autoregressively.\"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop sequence if too long\n",
        "        idx_cond = idx if idx.shape[1] <= model.config.block_size else idx[:, -model.config.block_size:]\n",
        "\n",
        "        # Get predictions\n",
        "        logits, _ = model(idx_cond, deterministic=True)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        if top_k is not None:\n",
        "            # Top-k sampling\n",
        "            top_logits, top_indices = jax.lax.top_k(logits, min(top_k, logits.shape[-1]))\n",
        "            probs = jax.nn.softmax(top_logits, axis=-1)\n",
        "            idx_next = jax.random.choice(\n",
        "                jax.random.PRNGKey(int(time.time())),\n",
        "                top_indices.shape[-1],\n",
        "                shape=(1,),\n",
        "                p=probs[0]\n",
        "            )\n",
        "            idx_next = top_indices[0, idx_next]\n",
        "        else:\n",
        "            # Regular sampling\n",
        "            probs = jax.nn.softmax(logits, axis=-1)\n",
        "            idx_next = jax.random.categorical(\n",
        "                jax.random.PRNGKey(int(time.time())),\n",
        "                logits,\n",
        "                axis=-1\n",
        "            )\n",
        "\n",
        "        # Append to sequence\n",
        "        idx = jnp.concatenate((idx, idx_next.reshape(1, 1)), axis=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "2exEgcS_QLq7"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_perplexity\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_perplexity\": []\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "num_epochs = 1\n",
        "eval_every = 500  # Evaluate every 100 steps\n",
        "\n",
        "# Main training loop\n",
        "total_steps = 0\n",
        "best_val_loss = float('inf')"
      ],
      "metadata": {
        "id": "30HBdmURQOdu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        train_step(gpt, optimizer, metrics, batch)\n",
        "        total_steps += 1\n",
        "\n",
        "        if total_steps > 0 and (total_steps % eval_every == 0):\n",
        "            # Log training metrics\n",
        "            train_metrics = metrics.compute()\n",
        "            for metric, value in train_metrics.items():\n",
        "                metrics_history[f\"train_{metric}\"].append(value)\n",
        "            metrics.reset()\n",
        "\n",
        "            # Validation\n",
        "            for val_batch in valid_dataloader:\n",
        "                eval_step(gpt, metrics, val_batch)\n",
        "\n",
        "            # Log validation metrics\n",
        "            val_metrics = metrics.compute()\n",
        "            for metric, value in val_metrics.items():\n",
        "                metrics_history[f\"val_{metric}\"].append(value)\n",
        "            metrics.reset()\n",
        "\n",
        "            # Print progress\n",
        "            print(\n",
        "                f\"[Epoch {epoch}][Step {total_steps}] \"\n",
        "                f\"Train Loss: {metrics_history['train_loss'][-1]:.4f} \"\n",
        "                f\"Train PPL: {metrics_history['train_perplexity'][-1]:.4f} | \"\n",
        "                f\"Val Loss: {metrics_history['val_loss'][-1]:.4f} \"\n",
        "                f\"Val PPL: {metrics_history['val_perplexity'][-1]:.4f}\"\n",
        "            )\n",
        "\n",
        "            # Save best model (you can implement saving logic here)\n",
        "            if val_metrics['loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['loss']\n",
        "                print(f\"New best validation loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EMP9As4FQU97",
        "outputId": "5419ffc1-9c79-4c12-db82-0d4f00caa42c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0][Step 500] Train Loss: 5.5761 Train PPL: 547.7780 | Val Loss: 5.1558 Val PPL: 221.7115\n",
            "New best validation loss: 5.1558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0][Step 1000] Train Loss: 4.6120 Train PPL: 104.0870 | Val Loss: 4.7349 Val PPL: 154.6977\n",
            "New best validation loss: 4.7349\n",
            "[Epoch 0][Step 1500] Train Loss: 4.2999 Train PPL: 76.1086 | Val Loss: 4.5557 Val PPL: 131.6437\n",
            "New best validation loss: 4.5557\n",
            "[Epoch 0][Step 2000] Train Loss: 4.0196 Train PPL: 57.4507 | Val Loss: 4.4569 Val PPL: 120.4873\n",
            "New best validation loss: 4.4569\n",
            "[Epoch 0][Step 2500] Train Loss: 3.7720 Train PPL: 44.7964 | Val Loss: 4.4164 Val PPL: 117.6189\n",
            "New best validation loss: 4.4164\n",
            "[Epoch 0][Step 3000] Train Loss: 3.4360 Train PPL: 32.0304 | Val Loss: 4.4019 Val PPL: 120.9069\n",
            "New best validation loss: 4.4019\n",
            "[Epoch 0][Step 3500] Train Loss: 3.0608 Train PPL: 22.1684 | Val Loss: 4.5267 Val PPL: 141.7111\n",
            "[Epoch 0][Step 4000] Train Loss: 2.5977 Train PPL: 14.1584 | Val Loss: 4.6910 Val PPL: 171.0228\n",
            "[Epoch 0][Step 4500] Train Loss: 2.0620 Train PPL: 8.2785 | Val Loss: 4.9290 Val PPL: 225.3462\n",
            "[Epoch 0][Step 5000] Train Loss: 1.5335 Train PPL: 4.8242 | Val Loss: 5.1843 Val PPL: 313.5801\n",
            "[Epoch 0][Step 5500] Train Loss: 1.1625 Train PPL: 3.2772 | Val Loss: 5.3932 Val PPL: 380.3051\n",
            "[Epoch 0][Step 6000] Train Loss: 0.9039 Train PPL: 2.5120 | Val Loss: 5.6012 Val PPL: 480.5926\n",
            "[Epoch 0][Step 6500] Train Loss: 0.7622 Train PPL: 2.1655 | Val Loss: 5.7261 Val PPL: 554.6519\n",
            "[Epoch 0][Step 7000] Train Loss: 0.6544 Train PPL: 1.9377 | Val Loss: 5.8261 Val PPL: 597.4398\n",
            "[Epoch 0][Step 7500] Train Loss: 0.5948 Train PPL: 1.8229 | Val Loss: 5.9212 Val PPL: 673.5679\n",
            "[Epoch 0][Step 8000] Train Loss: 0.5462 Train PPL: 1.7349 | Val Loss: 5.9718 Val PPL: 693.6426\n",
            "[Epoch 0][Step 8500] Train Loss: 0.5052 Train PPL: 1.6635 | Val Loss: 6.0484 Val PPL: 804.1441\n",
            "[Epoch 0][Step 9000] Train Loss: 0.4867 Train PPL: 1.6323 | Val Loss: 6.0420 Val PPL: 806.9131\n",
            "[Epoch 0][Step 9500] Train Loss: 0.4567 Train PPL: 1.5830 | Val Loss: 6.1558 Val PPL: 856.2457\n",
            "[Epoch 0][Step 10000] Train Loss: 0.4425 Train PPL: 1.5602 | Val Loss: 6.1532 Val PPL: 881.8196\n",
            "[Epoch 0][Step 10500] Train Loss: 0.4257 Train PPL: 1.5345 | Val Loss: 6.1980 Val PPL: 861.7120\n",
            "[Epoch 0][Step 11000] Train Loss: 0.4175 Train PPL: 1.5217 | Val Loss: 6.2172 Val PPL: 884.9273\n",
            "[Epoch 0][Step 11500] Train Loss: 0.4041 Train PPL: 1.5013 | Val Loss: 6.2037 Val PPL: 907.2004\n",
            "[Epoch 0][Step 12000] Train Loss: 0.3963 Train PPL: 1.4898 | Val Loss: 6.2044 Val PPL: 917.1686\n",
            "[Epoch 0][Step 12500] Train Loss: 0.3815 Train PPL: 1.4679 | Val Loss: 6.2465 Val PPL: 935.5539\n",
            "[Epoch 0][Step 13000] Train Loss: 0.3799 Train PPL: 1.4653 | Val Loss: 6.2598 Val PPL: 994.7725\n",
            "[Epoch 0][Step 13500] Train Loss: 0.3699 Train PPL: 1.4505 | Val Loss: 6.2648 Val PPL: 968.4508\n",
            "[Epoch 0][Step 14000] Train Loss: 0.3585 Train PPL: 1.4339 | Val Loss: 6.2475 Val PPL: 934.7157\n",
            "[Epoch 0][Step 14500] Train Loss: 0.3595 Train PPL: 1.4353 | Val Loss: 6.2468 Val PPL: 910.3489\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-97878cd54248>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtotal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36mupdate_context_manager_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_context_manager_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_context_manager_wrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/transforms/transforms.py\u001b[0m in \u001b[0;36mjit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     )\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mgraphdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_graph_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     out, output_state, output_graphdef = jitted_fn(\n\u001b[1;32m    360\u001b[0m       \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, node, *filters)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;34m\"'merge' was not called in-between the first and second call to 'split'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m       )\n\u001b[0;32m--> 969\u001b[0;31m     \u001b[0mgraphdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGraphState\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraphState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(x, idxmap)\u001b[0m\n\u001b[1;32m    385\u001b[0m   \u001b[0mrefmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRefMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m   \u001b[0mflat_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPathParts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStateLeaf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m   \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0midxmap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0msubgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodedef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0msubgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodedef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0msubgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodedef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0msubgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodedef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0msubgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodedef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0msubgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodedef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mnodedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_graph_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0msubgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodedef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_graph_flatten\u001b[0;34m(path, refmap, flat_state, node)\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m   \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_flatten_pytree\u001b[0;34m(pytree)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0mpytree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m   )\n\u001b[0;32m-> 1673\u001b[0;31m   \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_key_path_to_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0mpytree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m   )\n\u001b[0;32m-> 1673\u001b[0;31m   \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_key_path_to_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flax/nnx/nnx/graph.py\u001b[0m in \u001b[0;36m_key_path_to_key\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlattenedIndexKey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m   ):\n\u001b[0;32m-> 1658\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m       raise ValueError(\n\u001b[1;32m   1660\u001b[0m         \u001b[0;34mf'Invalid key: {key.key}. May be due to its type not being hashable or comparable.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/typing.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         if ((not getattr(cls, '_is_protocol', False) or\n\u001b[0;32m-> 1502\u001b[0;31m                 _is_callable_members_only(cls)) and\n\u001b[0m\u001b[1;32m   1503\u001b[0m                 issubclass(instance.__class__, cls)):\n\u001b[1;32m   1504\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/typing.py\u001b[0m in \u001b[0;36m_is_callable_members_only\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_callable_members_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;31m# PEP 544 prohibits using issubclass() with protocols that have non-method members.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_protocol_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/typing.py\u001b[0m in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__annotations__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_abc_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEXCLUDED_ATTRIBUTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"First Citizen:\\nYou \"\n",
        "prompt_tokens = jnp.array(tokenizer.encode(prompt))[None, :]\n",
        "\n",
        "# print(prompt_tokens.shape)\n",
        "pred_tokens = generate(gpt, idx=prompt_tokens, max_new_tokens=1024)"
      ],
      "metadata": {
        "id": "-TSOiioyQX9q"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(pred_tokens.tolist()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBJl4QPtGEIW",
        "outputId": "60307310-cc7a-4df5-e665-4b60e57b0236"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "You orrowed well said,arer; no, so did gates.\n",
            "\n",
            "Third Citizen:\n",
            "I am five hundred, and thoure'st happy to be logs,\n",
            " great and thy great kingdom to my death.\n",
            "\n",
            "Second Citizen:\n",
            "And so did I; and so did I.\n",
            "\n",
            "BRUTUS:\n",
            "What then, that want wasiron.\n",
            "\n",
            "CORIOLANUS:\n",
            "as son.\n",
            "\n",
            "MENENIUS:\n",
            "The matter?\n",
            "\n",
            "COMINIUS:\n",
            "Ay, nine, call,kindenative; hearts!\n",
            "\n",
            "CORIOLANUS:\n",
            " dimce have youSee how have youurrent made, sir?\n",
            "\n",
            "Both that amWhen, sir.\n",
            "\n",
            "MENENIUS:\n",
            "How! Was it we? we loved him but, like beasts\n",
            "And cowardly nobles, gave way unto your clusters,\n",
            "Who did hoot him out o' the city.\n",
            "\n",
            "COMINIUS:\n",
            "But I fear\n",
            "They'll roar him in again. Tullus Aufidius,\n",
            "The second name of men, obeys his points\n",
            "As if he were his officer: desperation\n",
            "Is all the policy, strength and defence,\n",
            "That Rome can make against them.\n",
            "\n",
            "MENENIUS:\n",
            "Here come the clusters.\n",
            "And is Aufidius with him? You are they\n",
            "That made the air unwh tellbalt with his\n",
            " true worse than trueoms.\n",
            "\n",
            "MENENIUS:\n",
            "I amShe's Go.\n",
            "\n",
            "First Senator:\n",
            "Now, as he made before, bereft his face.\n",
            "\n",
            "MENENIUS:\n",
            "The warm be the tribunes have been h patiently,\n",
            "To be Aufidius in grief?\n",
            "\n",
            "First Senator:\n",
            "Tut, tut,\n",
            "Hath he to be Fare Farewell;WillULE profound, society,\n",
            " society, be patience; I'll take it as apparent\n",
            "To dear Polixenes, and live,\n",
            "And say you have, her at the worth. You have made wind\n",
            "As semicBeunes could doubt, all doubt\n",
            "But that haste is conducted to your relish;\n",
            "And he shall awake his country, and his transparent\n",
            "GLOUGLOUCESTER:\n",
            "AThusuc track of the maidsous heir:\n",
            "But wherefore or lost his Caesarly have I then?\n",
            "\n",
            "BUCKINGHAM:\n",
            "I am cruel for the holy prince;\n",
            "A ribs of his ribs he would please you\n",
            "To Pomfret next blood, his brother's wife.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "With all our harts.\n",
            "\n",
            "BUCKINGHAM:\n",
            "My lord, he doth deny to come.\n",
            "\n",
            "KING RICHARD III:\n",
            " procure by the justice of his son should come.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Akeding Mund, might have might be weep,\n",
            "To see his in sickness from the whose wear swear,\n",
            "And may infected him from her face.\n",
            "But I, that she and would she person,--\n",
            "God bless him, I will not marry her, nor he faults,\n",
            "To prick thyWikipedia nothing; but,\n",
            " if thou wiltto him,toto thy is perform a maid\n",
            "In tyrannous cut from myNay, an hour,\n",
            "So far from the Keeper: must Iition,\n",
            "ition, pleasant mine my pleasant love's love\n",
            " resistedlile on myle sad months.\n",
            "\n",
            "First Musician:\n",
            " unequ, you know,\n",
            " compounds so, compounds to theChift theOutput,\n",
            " Menenius, a royal imperial; and, if\n",
            "w captivatesby, this is all.\n",
            "\n",
            "First Servingman:\n",
            "What a poor man so please you, sir, he has of the\n",
            " paved paved methought, that he would have, from the i' the i predecessors\n",
            " burden burden burden burdenrierieves, as if\n",
            "The buy why not theirwarning i' the oracle,\n",
            " love's tears do wash; a brace\n",
            "As but a reverence, woman, be four. tell me, what say you?\n",
            "Nay, but prit is a lady: this lady's nose\n",
            "When she theyard from theul, madam lady:\n",
            "re she we can gin\n",
            "toWe at thine enemy should heramed an;\n",
            "And therefore let her dowry do't at your graceranio's again.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "O, I can sullen,'twas hatred,\n",
            "To sa minister to sauiled and apace:\n",
            " kill him, I must go and fraud.\n",
            "\n",
            " RIGONUS:\n",
            "I swear to theimble underage of myHear him speak:\n",
            " i' the earth-place pluck\n",
            "He shall be a sand; that which again\n",
            "My formerMyMyMy ill suspicion.\n",
            "\n",
            "KING RICHARD II:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OESGXyyeIV0s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}